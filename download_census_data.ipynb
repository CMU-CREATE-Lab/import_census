{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import pandas as pd\n",
    "from utils.utils import SimpleThreadPoolExecutor\n",
    "import numpy as np\n",
    "from psql_utils.epsql import get_schema, get_table_name, sanitize_table_name\n",
    "from psql_utils import epsql\n",
    "\n",
    "@cache\n",
    "def engine():\n",
    "    return epsql.Engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_api_get(base_url, payload):\n",
    "    payload = payload.copy() # Don't modify the original\n",
    "    payload['key'] = open(\"secrets/census_api_key.txt\").read().strip()\n",
    "    response = requests.get(base_url, params=payload)\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code, response.text)\n",
    "        response.raise_for_status()\n",
    "    return pd.DataFrame(response.json()[1:], columns=response.json()[0])\n",
    "\n",
    "class CensusApiDataset():\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.schema_name = \"census\"\n",
    "        unsorted_vars = requests.get(f'https://api.census.gov/data/{dataset_name}/variables.json').json()['variables']\n",
    "        self.variables = dict(sorted(unsorted_vars.items()))\n",
    "        tables_build = defaultdict(dict)\n",
    "        for var_name, var_info in self.variables.items():\n",
    "            for table_name in set(var_info['group'].split(',')) - {'N/A'}:\n",
    "                tables_build[table_name][var_name]=var_info\n",
    "\n",
    "        self.tables = dict(sorted(tables_build.items()))\n",
    "        print(f\"{dataset_name}: found tables ({', '.join(self.tables.keys())})\")\n",
    "    \n",
    "    def get_data(self, fields: list[str], geometry_level: str, in_: str = \"\"):\n",
    "        if geometry_level == \"blockgroup\":\n",
    "            geometry_level = \"block group\"\n",
    "        assert(fields)\n",
    "        data: pd.DataFrame|None = None\n",
    "        shards: list[pd.DataFrame] = []\n",
    "        # Observe the census api limit of 50 fields per request, and make sure to add GEO_ID\n",
    "        # to each subrequest so we can join the results\n",
    "        for subfields in [fields[i:i + 49] for i in range(0, len(fields), 49)]:\n",
    "            if \"GEO_ID\" not in subfields:\n",
    "                subfields.append(\"GEO_ID\")                \n",
    "            payload = {\n",
    "                'get': ','.join(subfields),\n",
    "                'for': f'{geometry_level}:*'\n",
    "            }\n",
    "            if in_:\n",
    "                payload['in'] = in_\n",
    "        \n",
    "            subdata = census_api_get(f'https://api.census.gov/data/{self.dataset_name}', payload)\n",
    "\n",
    "            if data is None:\n",
    "                data = subdata\n",
    "            else:\n",
    "                # Join on all columns that are in both dataframes, as merge will not duplicate and add suffixes\n",
    "                # to any columns in the \"on\" argument.\n",
    "                # We assume that any duplicate columns (e.g. state, county) have the same data, and that GEO_ID\n",
    "                # in particular is specific enough to guarantee like records are combined.\n",
    "                join_columns = list(set(subdata.columns).intersection(set(data.columns)))\n",
    "                data = data.merge(subdata, on=join_columns)\n",
    "\n",
    "        assert(data is not None)\n",
    "\n",
    "        for col in data.columns:\n",
    "            var_info = self.variables.get(col)\n",
    "            if var_info:\n",
    "                dtypes = {\n",
    "                    'int': np.int32,\n",
    "                    'string': object,\n",
    "                    'float': np.float32,\n",
    "                    \n",
    "                }\n",
    "                new_dtype = dtypes[var_info['predicateType']]\n",
    "                if data[col].dtype != new_dtype:\n",
    "                    data[col] = data[col].astype(dtypes[var_info['predicateType']])\n",
    "    \n",
    "        if 'GEO_ID' not in fields:\n",
    "            data.drop(columns=['GEO_ID'], inplace=True)\n",
    "\n",
    "        if \"block group\" in data.columns:\n",
    "            data.rename(columns={\"block group\": \"blockgroup\"}, inplace=True)\n",
    "\n",
    "        if \"GEO_ID\" in data.columns:\n",
    "            data['geoid'] = data['GEO_ID'].str[9:]\n",
    "        return data\n",
    "    \n",
    "    @cache\n",
    "    def get_states(self):\n",
    "        states = self.get_data([\"NAME\", \"GEO_ID\"], \"state\")\n",
    "        return dict(sorted(zip(states[\"geoid\"], states.to_dict('records'))))\n",
    "\n",
    "    @cache\n",
    "    def get_counties(self):\n",
    "        counties = self.get_data([\"NAME\", \"GEO_ID\"], \"county\")\n",
    "        return dict(sorted(zip(counties[\"geoid\"], counties.to_dict('records'))))\n",
    "\n",
    "    def sql_table_name(self, table_name: str, geometry_level: str):\n",
    "        tokens = self.dataset_name.split(\"/\")\n",
    "        assert(len(tokens) == 3)\n",
    "        (year, dataset, subfile) = tokens\n",
    "        return sanitize_table_name(f\"{self.schema_name}.{dataset}{year}{subfile}_{table_name}_{geometry_level}\")\n",
    "    \n",
    "    # Download all records for table_name\n",
    "    def download_table(self, table_name: str, geometry_level: str):\n",
    "        sql_table_name = self.sql_table_name(table_name, geometry_level)\n",
    "        print(f\"Downloading {sql_table_name}\")\n",
    "        engine().execute(f\"CREATE SCHEMA IF NOT EXISTS {get_schema(sql_table_name)}\")\n",
    "\n",
    "        assert table_name in self.tables\n",
    "        assert geometry_level in ['block', 'blockgroup', 'tract', 'county', 'place', 'state']\n",
    "        fields = []\n",
    "        for var_name, var_info in self.tables[table_name].items():\n",
    "            fields.append(var_name)\n",
    "            fields += var_info['attributes'].split(',')\n",
    "        shards = []\n",
    "        if geometry_level == 'tract':\n",
    "            downloads = [{\n",
    "                \"sql\":f\"geoid between '{geoid}' and '{geoid}z'\",\n",
    "                \"in\":f\"state:{geoid}\"\n",
    "            } for geoid in self.get_states().keys()]\n",
    "        elif geometry_level in ['blockgroup', 'block']:\n",
    "            downloads = [{\n",
    "                \"sql\":f\"geoid between '{geoid}' and '{geoid}z'\",\n",
    "                \"in\":f\"state:{geoid[0:2]} county:{geoid[2:5]}\"\n",
    "            } for geoid in self.get_counties().keys()]\n",
    "        else:\n",
    "            downloads = [{}]\n",
    "        n_done = 0\n",
    "        for download in downloads:\n",
    "            sql = f\"SELECT count(*) from {sql_table_name}\"\n",
    "            if \"sql\" in download:\n",
    "                sql += f\" WHERE {download['sql']}\"\n",
    "            in_ = download.get(\"in\", \"\")\n",
    "            if engine().table_exists(sql_table_name) and (count := engine().execute_returning_value(sql)) > 0:\n",
    "                n_done += 1\n",
    "                #print(f\"{sql_table_name} {in_} already loaded ({count} records), skipping\")\n",
    "                continue\n",
    "            table = self.get_data(fields, geometry_level, in_)\n",
    "            table.to_sql(get_table_name(sql_table_name), engine().engine, schema=get_schema(sql_table_name), if_exists='append', index=False)\n",
    "            if not engine().table_has_primary_key(sql_table_name):\n",
    "                engine().execute(f'ALTER TABLE {sql_table_name} ADD PRIMARY KEY (geoid)')\n",
    "            print(f\"{sql_table_name} {in_} loaded {len(table)} records of {len(table.columns)} fields\")\n",
    "        if n_done:\n",
    "            print(f\"{sql_table_name} {n_done} of {len(downloads)} downloads previously completed\")\n",
    "        print(f\"{sql_table_name} is complete\")\n",
    "\n",
    "    def download_table_geometries(self, table_name: str):\n",
    "        for geometry_level in ['state', 'county', 'tract', 'blockgroup', 'block']:\n",
    "            self.download_table(table_name, geometry_level)\n",
    "\n",
    "    def download_tables_geometries(self, nthreads: int = 10):\n",
    "        pool = SimpleThreadPoolExecutor(nthreads)\n",
    "        print(f\"Downloading tables {', '.join(self.tables.keys())}\")\n",
    "        for table_name in self.tables.keys():\n",
    "            pool.submit(self.download_table_geometries, table_name)\n",
    "        pool.shutdown()\n",
    "\n",
    "#engine().execute(\"drop table census.dec2020pl_p2_tract\")\n",
    "\n",
    "for dataset in [\n",
    "    \"2020/dec/pl\",\n",
    "    \"2010/dec/sf1\", \"2010/dec/sf2\",\n",
    "    \"2000/dec/sf1\", \"2000/dec/sf2\", \"2000/dec/sf3\"]:\n",
    "    ds = CensusApiDataset(dataset)\n",
    "    ds.download_tables_geometries()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
