{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, sys, threading, time\n",
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import pandas as pd\n",
    "from utils.utils import SimpleThreadPoolExecutor, PrCall, ThCall\n",
    "import numpy as np\n",
    "from psql_utils.epsql import get_schema, get_table_name, sanitize_table_name, sanitize_column_names\n",
    "from psql_utils import epsql\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "import sqlalchemy.exc\n",
    "import psycopg2.errors\n",
    "\n",
    "\n",
    "engine_dict = {}\n",
    "def engine() -> epsql.Engine: \n",
    "    id = (os.getpid(), threading.get_ident())\n",
    "    if id not in engine_dict:\n",
    "        engine_dict[id] = epsql.Engine(verbose = False)\n",
    "    return engine_dict[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_api_get(base_url, payload):\n",
    "    payload = payload.copy() # Don't modify the original\n",
    "    payload['key'] = open(\"secrets/census_api_key.txt\").read().strip()\n",
    "    retries = 5\n",
    "    for retry in range(retries):\n",
    "        if retry:\n",
    "            print(f\"Retry {retry+1} of {retries} for GET {base_url} {payload}\")\n",
    "        response = None\n",
    "        try:\n",
    "            response = requests.get(base_url, params=payload)\n",
    "            # If it looks like a server issue, make exception now\n",
    "            if response.status_code // 100 not in (2, 4):\n",
    "                response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"During try {retry+1} of {retries} for GET {response and response.url}, received exception {e}\")\n",
    "            if retry == retries - 1:\n",
    "                print(\"Aborting since this is the last retry\")\n",
    "                raise\n",
    "            continue\n",
    "        if response.status_code == 200:\n",
    "            if retry:\n",
    "                print(f\"On retry {retry+1}, successful GET {response.url}\")\n",
    "            return pd.DataFrame(response.json()[1:], columns=response.json()[0])\n",
    "        if response.status_code // 100 == 4:\n",
    "            # 4xx errors are client errors, so don't retry\n",
    "            print(f\"During try {retry+1} of {retries} for GET {response.url}, aborting due to client error status code {response.status_code} {response.text}\")\n",
    "            response.raise_for_status()\n",
    "        # Otherwise, retry\n",
    "        time.sleep(5)\n",
    "    raise Exception(\"Should never get here\")\n",
    "\n",
    "class CensusApiDataset():\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.schema_name = \"census\"\n",
    "        unsorted_vars = requests.get(f'https://api.census.gov/data/{dataset_name}/variables.json').json()['variables']\n",
    "        patches = {\n",
    "            '2000/dec/sf1': {\n",
    "                'P001001': {'predicateType': 'int'},\n",
    "                'P004001': {'predicateType': 'int'}\n",
    "            },\n",
    "            '2000/dec/sf2': {\n",
    "                'HCT004001': {'predicateType': 'int'}\n",
    "            },\n",
    "            '2010/dec/sf1': {\n",
    "                'P001001': {'predicateType': 'int'},\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for col, patch in patches.get(dataset_name, {}).items():\n",
    "            for key, value in patch.items():\n",
    "                print(f\"Patching {dataset_name}[{repr(col)}][{repr(key)}]={repr(value)}\")\n",
    "                unsorted_vars[col][key] = value\n",
    "                \n",
    "        # if dataset_name in [\"2000/dec/sf1\", \"2010/dec/sf1\"] and 'predicateType' not in unsorted_vars['P001001']:\n",
    "        #     print(f\"Patching predicateType='int' for {dataset_name}.P001001\")\n",
    "        #     unsorted_vars['P001001']['predicateType'] = \"int\"\n",
    "        self.variables = dict(sorted(unsorted_vars.items()))\n",
    "        tables_build = defaultdict(dict)\n",
    "        for var_name, var_info in self.variables.items():\n",
    "            for table_name in set(var_info['group'].split(',')) - {'N/A'}:\n",
    "                tables_build[table_name][var_name]=var_info\n",
    "\n",
    "        self.tables: Dict[str, dict] = dict(sorted(tables_build.items()))\n",
    "        self.table_geom_levels = defaultdict(list)\n",
    "        for table_name in self.tables.keys():\n",
    "            self.table_geom_levels[table_name].append('state')\n",
    "            self.table_geom_levels[table_name].append('county')\n",
    "            if table_name.startswith('PCO') or table_name.startswith('HCO'):\n",
    "                continue\n",
    "            self.table_geom_levels[table_name].append('tract')\n",
    "            if table_name.startswith('PCT') or table_name.startswith('HCT'):\n",
    "                continue\n",
    "            self.table_geom_levels[table_name].append('blockgroup')\n",
    "            self.table_geom_levels[table_name].append('block')\n",
    "\n",
    "        self.bad_cols = {}\n",
    "        print(f\"{dataset_name}: found tables ({', '.join(self.tables.keys())})\")\n",
    "    \n",
    "    def get_data(self, fields: list[str], geometry_level: str, in_: str = \"\"):\n",
    "        if geometry_level == \"blockgroup\":\n",
    "            geometry_level = \"block group\"\n",
    "        assert(fields)\n",
    "        payload = {\n",
    "            'get': ','.join(fields),\n",
    "            'for': f'{geometry_level}:*'\n",
    "        }\n",
    "        if in_:\n",
    "            payload['in'] = in_\n",
    "        \n",
    "        data = census_api_get(f'https://api.census.gov/data/{self.dataset_name}', payload)\n",
    "\n",
    "        conversion_exceptions = []\n",
    "\n",
    "        for col in data.columns:\n",
    "            var_info = self.variables.get(col)\n",
    "            if var_info:\n",
    "                dtypes = {\n",
    "                    'int': pd.Int32Dtype(), # NA-able (nullable) integer type\n",
    "                    'string': object,\n",
    "                    'float': np.float32,\n",
    "                }\n",
    "                if 'predicateType' not in var_info:\n",
    "                    raise RuntimeError(\n",
    "                        f\"var_info for {self.dataset_name}.{col} is missing predicateType\\n\"\n",
    "                        f\"var_info = {var_info}\"\n",
    "                    )\n",
    "\n",
    "                new_dtype = dtypes[var_info['predicateType']]\n",
    "                if data[col].dtype != new_dtype:\n",
    "                    required_type = dtypes[var_info['predicateType']]\n",
    "                    try:\n",
    "                        data[col] = data[col].astype(required_type)\n",
    "                    except TypeError as e:\n",
    "                        self.bad_cols[col] = data[col]\n",
    "                        conversion_exceptions.append(\n",
    "                            f\"Cannot convert {self.dataset_name}.{col} to type {required_type}\\n\"\n",
    "                            f\"(Column stored as self.bad_cols[{col}] for developer inspection)\\n\"\n",
    "                            f\"Source data from census API contains:\\n\"\n",
    "                            f\"{data[col].map(lambda x: type(x)).value_counts().to_string()}\\n\"\n",
    "                            f\"(Exception: {e})\")\n",
    "                null_count = data[col].isna().sum()\n",
    "                if null_count:\n",
    "                    raise RuntimeError(f\"{self.dataset_name}.{col} {geometry_level} contains {null_count} NULLs of {len(data[col])} values\")\n",
    "    \n",
    "        if conversion_exceptions:\n",
    "            raise RuntimeError(\"----------------\\n\".join(conversion_exceptions))\n",
    "        \n",
    "        if \"block group\" in data.columns:\n",
    "            data.rename(columns={\"block group\": \"blockgroup\"}, inplace=True)\n",
    "\n",
    "        if \"GEO_ID\" in data.columns:\n",
    "            data['geoid'] = data['GEO_ID'].str[9:]\n",
    "        return data\n",
    "    \n",
    "    @cache\n",
    "    def get_states(self):\n",
    "        states = self.get_data([\"NAME\", \"GEO_ID\"], \"state\")\n",
    "        return dict(sorted(zip(states[\"geoid\"], states.to_dict('records'))))\n",
    "\n",
    "    @cache\n",
    "    def get_counties(self):\n",
    "        counties = self.get_data([\"NAME\", \"GEO_ID\"], \"county\")\n",
    "        return dict(sorted(zip(counties[\"geoid\"], counties.to_dict('records'))))\n",
    "\n",
    "    def sql_table_name(self, table_name: str, geometry_level: str):\n",
    "        tokens = self.dataset_name.split(\"/\")\n",
    "        assert(len(tokens) == 3)\n",
    "        (year, dataset, subfile) = tokens\n",
    "        return sanitize_table_name(f\"{self.schema_name}.{dataset}{year}{subfile}_{table_name}_{geometry_level}\")\n",
    "    \n",
    "    # Download all records for table_name\n",
    "    def download_table(self, table_name: str, geometry_level: str):\n",
    "        sql_table_name = self.sql_table_name(table_name, geometry_level)\n",
    "        #print(f\"Downloading {sql_table_name}\")\n",
    "        engine().execute(f\"CREATE SCHEMA IF NOT EXISTS {get_schema(sql_table_name)}\")\n",
    "\n",
    "        assert table_name in self.tables\n",
    "        assert geometry_level in self.table_geom_levels[table_name]\n",
    "        # fields = []\n",
    "        # for var_name, var_info in self.tables[table_name].items():\n",
    "        #     fields.append(var_name)\n",
    "        #     fields += var_info['attributes'].split(',')\n",
    "        # shards = []\n",
    "        if geometry_level in ['tract', 'blockgroup', 'block']:\n",
    "            downloads = [{\n",
    "                \"sql\":f\"geoid between '{geoid}' and '{geoid}z'\",\n",
    "                \"in\":f\"state:{geoid} county:*\"\n",
    "            } for geoid in self.get_states().keys()]\n",
    "        else:\n",
    "            downloads = [{}]\n",
    "        \n",
    "        # Create tqdm progress bar that's initially not displayed\n",
    "        pbar =  None\n",
    "\n",
    "        for i, download in enumerate(downloads):\n",
    "            sql = f\"SELECT geoid from {sql_table_name}\"\n",
    "            if \"sql\" in download:\n",
    "                sql += f\" WHERE {download['sql']}\"\n",
    "            sql += \" LIMIT 1\"\n",
    "            in_ = download.get(\"in\", \"\")\n",
    "            if engine().table_exists(sql_table_name) and len(engine().execute_returning_dicts(sql)) > 0:\n",
    "                #print(f\"{sql_table_name} {in_} already loaded ({count} records), skipping\")\n",
    "                if pbar is not None:\n",
    "                    pbar.update()\n",
    "                continue\n",
    "\n",
    "            if pbar is None:\n",
    "                pbar = tqdm(total=len(downloads), desc=sql_table_name, initial=i)\n",
    "\n",
    "            PrCall(self.get_data_and_insert, table_name, geometry_level, sql_table_name, in_).value()\n",
    "            pbar.update()\n",
    "    \n",
    "        self.add_primary_key(sql_table_name)\n",
    "\n",
    "        if pbar is not None:\n",
    "            pbar.close()\n",
    "\n",
    "    def get_data_and_insert(self, table_name, geometry_level, sql_table_name, in_):\n",
    "        table = self.get_data([f\"group({table_name})\"], geometry_level, in_)\n",
    "        sanitize_column_names(table, inplace=True)\n",
    "        try:\n",
    "            table.to_sql(get_table_name(sql_table_name), engine().engine, schema=get_schema(sql_table_name), if_exists='append', index=False)\n",
    "        except (sqlalchemy.exc.IntegrityError, psycopg2.errors.UniqueViolation) as e:\n",
    "            print(f\"While get_data_and_insert for {sql_table_name}, in_ {in_}, got exception {e}\", flush=True)\n",
    "            raise e\n",
    "        self.add_primary_key(sql_table_name)\n",
    "\n",
    "    def add_primary_key(self, sql_table_name):\n",
    "        if not engine().table_has_primary_key(sql_table_name):\n",
    "            try:\n",
    "                engine().execute(f'ALTER TABLE {sql_table_name} ADD PRIMARY KEY (geoid)')\n",
    "            except (sqlalchemy.exc.IntegrityError, psycopg2.errors.UniqueViolation) as e:\n",
    "                if engine().table_column_exists(sql_table_name, 'popgroup'):\n",
    "                    engine().execute(f'ALTER TABLE {sql_table_name} ADD PRIMARY KEY (geoid, popgroup)')\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Table {sql_table_name} has duplicate geoids, but no popgroup column.\")\n",
    "\n",
    "    def download_table_geometries(self, table_name: str):\n",
    "        for geometry_level in self.table_geom_levels[table_name]:\n",
    "            self.download_table(table_name, geometry_level)\n",
    "\n",
    "    def download_tables_geometries(self, nthreads: int = 15):\n",
    "        print(f\"Downloading tables {', '.join(self.tables.keys())}\", flush=True)\n",
    "        if nthreads == 1:\n",
    "            for table_name in self.tables.keys():\n",
    "                self.download_table_geometries(table_name)\n",
    "        else:\n",
    "            pool = SimpleThreadPoolExecutor(nthreads)\n",
    "            for table_name in self.tables.keys():\n",
    "                 pool.submit(self.download_table_geometries, table_name)\n",
    "            pool.shutdown(tqdm=tqdm(desc=self.dataset_name, colour=\"red\"))\n",
    "\n",
    "def display_storage():\n",
    "    bar = None\n",
    "    while True:\n",
    "        size = engine().list_schema_sizes().query('schema_name == \"census\"')['size_mb'].iloc[0]*1e6\n",
    "        if bar is None:\n",
    "            bar = tqdm(desc=\"census schema size\", colour=\"red\", unit=\"B\", initial=size, unit_scale=True)\n",
    "        else:\n",
    "            bar.update(size - bar.n)\n",
    "        time.sleep(60)\n",
    "\n",
    "#engine().execute(\"drop table census.dec2020pl_p2_tract\")\n",
    "\n",
    "#ds = CensusApiDataset(\"2010/dec/sf1\")\n",
    "\n",
    "ThCall(display_storage)\n",
    "\n",
    "for dataset in [\n",
    "    #\"2020/dec/pl\",\n",
    "    #\"2010/dec/sf1\", \n",
    "    #\"2010/dec/sf2\",\n",
    "    \"2000/dec/sf1\", \"2000/dec/sf2\", \"2000/dec/sf3\"]:\n",
    "    ds = CensusApiDataset(dataset)\n",
    "    ds.download_tables_geometries(nthreads=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table in engine().list_tables(\"census\"):\n",
    "#     if table.startswith(\"dec2020\"):\n",
    "#         engine().execute(\"drop table census.\" + table)\n",
    "#     else:\n",
    "#         print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine().execute_returning_df('select \"POPGROUP\" from census.dec2010sf2_hct1_state limit 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d02dc5a46efe9a16a7dd68f5fbf604eda098a5267d007165c3b7d3bf3fe9764"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
